{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacd746d-d1c4-4427-b6e4-4d2452cef7eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import libraries for time-series analysis\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from pandas import read_csv, set_option\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer, RobustScaler\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "import xgboost\n",
    "from xgboost import plot_importance, XGBClassifier, XGBRegressor\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "import shap\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import SparsePCA\n",
    "from tqdm.autonotebook import tqdm\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import MiniBatchDictionaryLearning\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.manifold import Isomap\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set(style='whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48784fe-163d-480e-8967-04d81c8c3b94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdbe798-a62c-4de8-a38f-b40d4bae7a1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define time conversion custom function for timestamps in string format -> native timestamps in the csv file\n",
    "def datetime_parser(timestamp_str):\n",
    "    return datetime.datetime.strptime(timestamp_str, \"%Y-%m-%d %H:%M:%S%z\")\n",
    "\n",
    "# define path for train_df1 dataset\n",
    "path = './data/train_df1.csv'\n",
    "\n",
    "# for time series data, the datetime is always the index\n",
    "train_df1 = pd.read_csv(path,\n",
    "                        parse_dates=[0], # to be decided later\n",
    "                        date_parser = datetime_parser,\n",
    "                        index_col = 'Timestamp')    \n",
    "\n",
    "train_df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2dad98-b364-4035-9707-2db6ac7b321f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import the test_df1 csv file \n",
    "path = './data/test_df1.csv'\n",
    "\n",
    "# for time series data, the datetime is always the index\n",
    "test_df1 = pd.read_csv(path,\n",
    "                       parse_dates=[0], # to be decided later\n",
    "                       date_parser = datetime_parser,\n",
    "                       index_col = 'Timestamp')    \n",
    "\n",
    "test_df1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ce07b7-da7c-462f-afe2-c550e49ba426",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da837fc8-fdc1-446f-b6e9-a0fb8565c76c",
   "metadata": {},
   "source": [
    "#### 3.1 Baseline Model Features\n",
    "Plot a baseline correlation feature matrix to visualize baseline dataframe feature correlation to signal (target variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a1890c-66b0-44a4-895b-cfa3248204d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cutomized function for the correlation of baseline features with target variable\n",
    "def corrMat(df, \n",
    "            target='', \n",
    "            figsize=(9,0,5), \n",
    "            ret_id=False):\n",
    "    corr_mat = df.corr().round(3);\n",
    "    corr_mat = corr_mat.transpose()\n",
    "    corr = corr_mat.loc[:, df.columns == target].transpose().copy()\n",
    "    \n",
    "    # define a default colormap\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "    \n",
    "    if (ret_id is False):\n",
    "        f, ax = plt.subplots(figsize = figsize)\n",
    "        sns.heatmap(corr,\n",
    "                    vmin=-1,\n",
    "                    vmax=1, \n",
    "                    center=0,\n",
    "                    cmap=cmap,\n",
    "                    square=False,\n",
    "                    lw=2, \n",
    "                    annot=True,\n",
    "                    cbar=False)\n",
    "        plt.title(f'Feature Correlation to {target}')\n",
    "        \n",
    "    if (ret_id):\n",
    "        return corr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df41f05f-0534-469b-a766-6288149d1110",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corrMat(train_df1,'Signal',figsize=(7,0.5)) # baseline dataframe feature correlation to Signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf7af18-2d98-42a2-857d-7dc5db88c7f9",
   "metadata": {},
   "source": [
    "All features have weak negative linear correlations to the target variable (buy/sell signals). \n",
    "\n",
    "This is suggestive of a number of things:\n",
    "`open`, `high`, `low`, `close`, `volume_(BTC)`, `volume_(Currency)`, `weighted_price`, `SMA`, `LMA` may have (1) high non-linearity, (2) stable ocsillation relative to stationary value (circular scatter), or (3) they are not the most ideal to model the target variable `signal` and can be improved so the attention shifts to feature engineering. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c74b2e-cb35-4ba7-b262-b76e3348cd47",
   "metadata": {},
   "source": [
    "#### 3.2 Feature Engineering - Conversion of Variables into Traditional Technical Indicators\n",
    "Technical Indicators to measure performance of assets : \n",
    "- Moving Averages (20MA, 50MA, 250MA)\n",
    "- Exponential Moving Averages (10, 30, 200)\n",
    "- Momentum\n",
    "- Relative Strength Index (RSI)\n",
    "- Stochastic Oscillators (Slow)\n",
    "- Stochastic Oscillators (Fast) \n",
    "\n",
    "These engineered features will be introduced into the feature matrix. The results will show us which features have the most significant impact on the model's performance, if any. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23df568-fe8c-411e-aa07-1085a4c09028",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df2 = train_df1.copy()  # duplicate dataframes & add features to them\n",
    "test_df2 = test_df1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9333d1e-fb44-40d6-9817-2ae22c18606b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# comversion formula for moving averages (20MA, 50MA, 250MA)\n",
    "# the moving average provides an indication of the trend of the price movement by reducing the amount of noise\n",
    "\n",
    "def ma(df, n):\n",
    "    return pd.Series(df['Close'].rolling(n, min_periods=n).mean(), name='MA_'+str(n))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96d1dff-248f-49df-bee9-76a15315758b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# conversion formula for exponential moving averages (10, 30, 200)\n",
    "\n",
    "def exp_ma(df, n):\n",
    "    return pd.Series(df['Close'].ewm(span=n, min_periods=n).mean(), name='EMA_'+str(n))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0401313-f83c-4f50-bd76-367dfc8a3a1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# conversion formula for price momentum\n",
    "\n",
    "def mom(df, n):\n",
    "    return pd.Series(df.diff(n), name='Momentum_'+str(n))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844efe78-fae5-435d-ba25-a27ab9e13acc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# conversion formula for relative strength index (RSI)\n",
    "# a momentum indicator that measures the magnitude of recent price changes to evaluate overbought or oversold conditions in the price of a stock or other asset. Ranging from [1, 100].\n",
    "# if the asset -> 70 (asset deemed overbought)\n",
    "# if the asset -> 30 (asset getting undersold and undervalued)\n",
    "\n",
    "def rsi(df, period):\n",
    "    delta = df.diff().dropna()\n",
    "    u = delta * 0;\n",
    "    d = u.copy()\n",
    "    u[delta > 0] = delta[delta > 0];\n",
    "    d[delta < 0] = -delta[delta < 0];\n",
    "    u[u.index[period-1]] = np.mean( u[:period] )  # the first value is the sum of average gains\n",
    "    u.drop(u.index[:(period-1)])\n",
    "    d[d.index[period-1]] = np.mean( d[:period] ) # the first valud is the sum of average losses\n",
    "    d = d.drop(d.index[:(period-1)])\n",
    "    rs = u.ewm(com=period-1, adjust=False).mean() / d.ewm(com=period-1, adjust=False).mean()\n",
    "    return 100 - 100 / (1 + rs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6e8e9f-b62f-4769-a4ff-f295c50138af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# conversion formula for stochastic oscillators - slow/fast indicator\n",
    "# a stochastic oscillator is a momentum indicator comparing a particular closing price of a security to a range of its prices over a period of time (%K/%D)\n",
    "\n",
    "def sto(close, low, high, n,id): \n",
    "    stok = ((close - low.rolling(n).min()) / (high.rolling(n).max() - low.rolling(n).min())) * 100\n",
    "    if(id is 0):\n",
    "        return stok\n",
    "    else:\n",
    "        return stok.rolling(3).mean()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6de5c5e-5b0b-4995-83db-0a58df0448b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9307f6-91ca-495c-8212-e971040639f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# visualize overall asset price history during training data period and the associated buy/sell signals\n",
    "    \n",
    "# plot n verticle subplots\n",
    "def plot_vsubplots(ldf,\n",
    "                   lst,\n",
    "                   title='',\n",
    "                   nplots=None,\n",
    "                   lw_id=None,\n",
    "                   size=[400,1000]):\n",
    "\n",
    "    # lw_id list of line widths if added\n",
    "        \n",
    "    assert(nplots is not None) \n",
    "    fig = make_subplots(rows=nplots,\n",
    "                        shared_xaxes=True)\n",
    "    ii=-1\n",
    "    for i in lst:\n",
    "        ii+=1\n",
    "        fig.add_trace(go.Scatter(x=ldf.index,\n",
    "                                 y=ldf[lst[ii]], \n",
    "                                 mode='lines',\n",
    "                                 name=lst[ii],\n",
    "                                 line=dict(width=lw_id[ii])), \n",
    "                      row=ii+1, \n",
    "                      col=1) \n",
    "\n",
    "    fig.update_layout(height=size[0],\n",
    "                      width=size[1],\n",
    "                      template='plotly_white',\n",
    "                      title=title,\n",
    "                      margin=dict(l=50,\n",
    "                                  r=80,\n",
    "                                  t=50,\n",
    "                                  b=40));\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cac2a3c-64da-4d91-a917-8d67f521dac2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01f4007-e6e4-498e-8dcc-7dfb62aef83d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# customize function to plot trends across time with line plot\n",
    "def plot_line(ldf, \n",
    "              lst, \n",
    "              title='',\n",
    "              sec_id=None,\n",
    "              size=[350,1000]):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to plot trends across time with a line plot.\n",
    "    \n",
    "        Parameters:\n",
    "\n",
    "        (1) ldf : dataframe \n",
    "                The DataFrame containing the data to plot.\n",
    "\n",
    "        (2) lst : list of str\n",
    "                A list of column names to plot.\n",
    "\n",
    "        (3) title : str, optional \n",
    "                The title of the plot - default is an empty string.\n",
    "\n",
    "        (4) sec_id : list of bool, optional \n",
    "                A list of boolean values indicating whether to activate subplots; \n",
    "                Must be the same length as lst - default is None.\n",
    "\n",
    "        (5) size : list of int, optional \n",
    "                The size of the plot as [height, width] - default is [350, 1000].\n",
    "            \n",
    "    \"\"\"\n",
    "        \n",
    "    # if sec_id is provided, we create a subplot with secondary y-axis\n",
    "    if(sec_id is not None):\n",
    "        fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "    else:\n",
    "        fig = go.Figure() # otherwise, create a simple figure without subplots \n",
    "        \n",
    "    # check if lst contains more than one element\n",
    "    if(len(lst) is not 1): # use '!=' instead of 'is not' for integer comparison \n",
    "        ii =-1 # initialize an index for iterating through lst\n",
    "        for i in lst:\n",
    "            ii +=1 # increment index\n",
    "            if(sec_id != None):\n",
    "                # add a trace with a secondary y-axis if sec_id is provided and matches index\n",
    "                fig.add_trace(go.Scatter(x=ldf.index, \n",
    "                                         y=ldf[lst[ii]], \n",
    "                                         mode='lines', \n",
    "                                         name=lst[ii], \n",
    "                                         line=dict(width=2.0)), \n",
    "                              secondary_y=sec_id[ii])\n",
    "            else:\n",
    "                # add a trace to the figure without secondary y-axis\n",
    "                fig.add_trace(go.Scatter(x=ldf.index, \n",
    "                                         y=ldf[lst[ii]], \n",
    "                                         mode='lines', \n",
    "                                         name=lst[ii], \n",
    "                                         line=dict(width=2.0)))\n",
    "    else:\n",
    "        # if lst contains only one element, add a simple trace\n",
    "        fig.add_trace(go.Scatter(x=ldf.index, \n",
    "                                 y=ldf[lst[0]],\n",
    "                                 mode='lines',\n",
    "                                 name=lst[0],\n",
    "                                 line=dict(width=2.0)))\n",
    "\n",
    "    # update the layout of the figure with the specific size and title\n",
    "    fig.update_layout(height=size[0],\n",
    "                      width=size[1],\n",
    "                      template='plotly_white',\n",
    "                      title=title,\n",
    "                      margin=dict(l=50,\n",
    "                                  r=80,\n",
    "                                  t=50,\n",
    "                                  b=40));\n",
    "    \n",
    "    # display the plot  \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5ae0f5-214e-47da-875a-1ec1f387d079",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# customised function to create all technical indicators using embedded conversion formulas\n",
    "# with the option to plot the training data (technical indicators over time)\n",
    "\n",
    "plot_period = slice('2019-7-7 0:00','2019-7-7 8:00')  \n",
    "\n",
    "def technical_indicators(ldf, tr_id = True):\n",
    "    ''' Moving Average '''\n",
    "    ldf['MA21'] = ma(ldf,10)\n",
    "    ldf['MA63'] = ma(ldf, 30)\n",
    "    ldf['MA252'] = ma(ldf, 200)\n",
    "    lst_MA = ['MA21','MA63','MA252']\n",
    "\n",
    "    ''' Exponentially Weighted Moving Average '''\n",
    "    ldf['EMA10'] = exp_ma(ldf, 10)\n",
    "    ldf['EMA30'] = exp_ma(ldf, 30)\n",
    "    ldf['EMA200'] = exp_ma(ldf, 200)\n",
    "    lst_EMA = ['EMA10','EMA30','EMA200']\n",
    "\n",
    "    ''' Momentum '''\n",
    "    ldf['MOM10'] = mom(ldf['Close'], 10)\n",
    "    ldf['MOM30'] = mom(ldf['Close'], 30)\n",
    "    lst_MOM = ['MOM10','MOM30']\n",
    "\n",
    "    ''' Relative Strength Index / RSI '''\n",
    "    ldf['RSI10'] = rsi(ldf['Close'], 10)\n",
    "    ldf['RSI30'] = rsi(ldf['Close'], 30)\n",
    "    ldf['RSI200'] = rsi(ldf['Close'], 200)\n",
    "    lst_RSI = ['RSI10','RSI30','RSI200']\n",
    "\n",
    "    ''' Slow Stochastic Oscillators '''\n",
    "    ldf['%K10'] = sto(ldf['Close'], ldf['Low'], ldf['High'],5,0)\n",
    "    ldf['%K30'] = sto(ldf['Close'], ldf['Low'], ldf['High'],10,0)\n",
    "    ldf['%K200'] = sto(ldf['Close'], ldf['Low'], ldf['High'], 20,0)\n",
    "    lst_pK = ['%K10','%K30','%K200']\n",
    "\n",
    "    ''' Fast Stochastic Oscillators '''\n",
    "    ldf['%D10'] = sto(ldf['Close'], ldf['Low'], ldf['High'], 10,1)\n",
    "    ldf['%D30'] = sto(ldf['Close'], ldf['Low'], ldf['High'], 30,1)\n",
    "    ldf['%D200'] = sto(ldf['Close'], ldf['Low'], ldf['High'], 200,1)\n",
    "    lst_pD = ['%D10','%D30','%D200']\n",
    "    \n",
    "    # plot dataset\n",
    "    if(tr_id):\n",
    "        plot_line(ldf.loc[plot_period,lst_MA],lst_MA,title='Moving Average (window=21,63,252)')\n",
    "        plot_line(ldf.loc[plot_period,lst_EMA],lst_EMA,title='Exponential Moving Average (window=10,30,200)')\n",
    "        plot_line(ldf.loc[plot_period,lst_MOM],lst_MOM,title='Momentum')\n",
    "        plot_line(ldf.loc[plot_period,lst_RSI],lst_RSI,title='Relative Strength Index')\n",
    "        plot_line(ldf.loc[plot_period,lst_pK],lst_pK,title='Stochastic Oscillators (slow)')\n",
    "        plot_line(ldf.loc[plot_period,lst_pD],lst_pD,title='Stochastic Oscillators (fast)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce87ec3-a1d2-43de-8a00-eea4741edf35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "technical_indicators(train_df2) # add technical features to training set\n",
    "technical_indicators(test_df2,tr_id=False) # add technical features to test set, no need to plot trends in test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39499a6-ebde-423e-9b33-e86ab590db81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ensure that technical indicators have been added to the training / test dataset as additional features\n",
    "train_df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc04a403-177a-4c53-ab04-321c6dc4ebc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047eb208-9425-4f19-91a5-8dd39abbb3a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# original datasets without technical indicators added as additional features\n",
    "train_df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a2a692-6085-4c41-9c17-f988d2436d93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c85ef93-2f64-4a78-8641-afd4a281bf54",
   "metadata": {},
   "source": [
    "#### 3.3 Updated Baseline Model Features\n",
    "Plot a updated baseline correlation feature matrix to visualize baseline dataframe feature correlation to signal (target variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cafabd7-844b-4b9c-a569-0700f5e02376",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# updated feature linear correlation matrix\n",
    "corrMat(train_df2,'Signal',figsize=(20,0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5e2263-0ff6-4e14-90f8-55334d522157",
   "metadata": {},
   "source": [
    "* engineered features were more linearly correlated to target variable (signal)\n",
    "* original baseline features may not be very useful in the prediction of signals\n",
    "* linear correlation between engineered features and target variable is significant and not too high (risk of overfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a840ac4-6b08-425e-9626-baa62a55f2b8",
   "metadata": {},
   "source": [
    "#### 3.4 Feature Selection\n",
    "- Features with high linear correlation have high predictive power, to be included in the training of the predictive model\n",
    "- Remove features with low linear correlation to target variable, to prevent the introduction of Noise and Dimensionality - Poor Generalization & Lower Model Efficiency\n",
    "<br>\n",
    "(1) Inclusion of redundant features will cause model to capture noise rather than true patterns\n",
    "<br>\n",
    "(2) Curse of Dimensionality - increased dimensionality reduces model efficiency, more computational resources needed for high dimensional feature spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb99d569-ee9e-491c-b31f-317ac992c301",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# drop the original features with low predictive power and low feature importance\n",
    "# dropping redundant features that will introduce noise and dimensionality \n",
    "# 'Open', 'High', 'Low', 'Close', 'Volume_(BTC)', 'Volume_(Currency)', 'Weighted_Price', 'SMA', 'LMA'\n",
    "# drop features with low linear correlation + features that are redundant based on domain knowledge\n",
    "train_df2.drop(['Open','High', 'Low', 'Volume_(Currency)', 'SMA', 'LMA'], axis=1, inplace=True)\n",
    "train_df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f01bb1-47c4-4d91-b971-8b16f5729d3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df2.drop(['Open','High','Low','Volume_(Currency)', 'SMA', 'LMA'], axis=1, inplace=True)\n",
    "test_df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3cd816-e0f3-4563-9108-0858e4420c58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# customize function for plotting the percentage of missing values\n",
    "\n",
    "def bar_plot(x,\n",
    "             y, \n",
    "             palette_len,\n",
    "             title = 'Missing Values (%)',\n",
    "             xlim = None,\n",
    "             ylim = None,\n",
    "             xticklabels = None,\n",
    "             yticklabels = None,\n",
    "             xlabel = None,\n",
    "             ylabel = None,\n",
    "             figsize = (10,4),\n",
    "             axis_grid ='y'):\n",
    "    \n",
    "    cmap = sns.color_palette(\"plasma\")\n",
    "    fig, ax = plt.subplots(figsize = figsize)\n",
    "    plt.title(title, size=15, fontweight='bold')\n",
    "    \n",
    "    for i in ['top', 'right', 'bottom', 'left']:\n",
    "        ax.spines[i].set_color('black')\n",
    "        \n",
    "    ax.spines['top'].set_visible(True)\n",
    "    ax.spines['right'].set_visible(True)\n",
    "    ax.spines['bottom'].set_visible(True)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    \n",
    "    sns.barplot(x = x,\n",
    "                y = y,\n",
    "                palette = cmap, \n",
    "                ax = ax,\n",
    "                edgecolor = 'black')\n",
    "    \n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "    ax.set_xticklabels(xticklabels)\n",
    "    ax.set_yticklabels(yticklabels)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    ax.grid(axis = axis_grid, ls = '--', alpha = 0.9)\n",
    "    \n",
    "    plt.show\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063efd65-eb74-416d-a9f2-6a895c48607a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check updated training dataset for missing values\n",
    "# plot percentage of missing values for each column\n",
    "nan_values_tr2 = ((train_df2.isnull().sum() / len(train_df2)) * 100).sort_values(ascending=True)\n",
    "\n",
    "bar_plot(x = nan_values_tr2, \n",
    "         y = nan_values_tr2.index, \n",
    "         palette_len = nan_values_tr2.index, \n",
    "         xlim = (0,1),\n",
    "         xticklabels = range(0,10),\n",
    "         yticklabels = nan_values_tr2.index,\n",
    "         figsize = (10,5), \n",
    "         axis_grid = 'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bcea66-3a21-41cb-bfb0-684ab46b5581",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check updated testing dataset for missing values\n",
    "# plot percentage of values that are NA for each column\n",
    "nan_values_te2 = ((test_df2.isnull().sum() / len(test_df2)) * 100).sort_values(ascending=True)\n",
    "\n",
    "bar_plot(x = nan_values_te2,\n",
    "         y = nan_values_te2.index,\n",
    "         palette_len = nan_values_te2.index, \n",
    "         xlim = (0,1),\n",
    "         xticklabels = range(0,10),\n",
    "         yticklabels = nan_values_te2.index,\n",
    "         figsize = (10,5), \n",
    "         axis_grid = 'x')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21727623-b456-4363-9351-ec0e3466058d",
   "metadata": {},
   "source": [
    "There are missing values in the training and testing datasets,records to be removed since they make up only an insignificant percentage of the total count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46356c1-d145-412a-8075-47b4c0b65ea1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# drop the rows with missing values in train_df2\n",
    "train_df2 = train_df2.dropna(axis=0,\n",
    "                             inplace=False) \n",
    "train_df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e62531-9f9a-468a-b221-ef8d57896569",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df2.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b3a3fc-7bd7-4d47-9e3a-93bbaf5b9529",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# drop the rows with missing values in test_df2\n",
    "test_df2 = test_df2.dropna(axis=0,\n",
    "                           inplace=False)\n",
    "test_df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c04dc45-09a1-46af-b311-8e65c44ba07a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df2.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c315e0-de3c-4aec-bab4-a87b4b842013",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save transformed train_df2 for model generation and evaluation\n",
    "train_df2.to_csv('./data/train_df2.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a010b4-d439-4ac0-8346-1c69ca446e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save transformed test_df2 for model generation and evaluation\n",
    "test_df2.to_csv('./data/test_df2.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
